# -*- coding: utf-8 -*-
"""utils.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UCG1J86UG10zSH-j0FAab6rPFJcXUjD_
"""

from googletrans import Translator
import re 
import unicodedata
import spacy
from nltk.tokenize import word_tokenize
import wordninja
from spacy.lang.en.stop_words import STOP_WORDS
import nltk
from nltk.corpus import words

def split(text):
    token = word_tokenize(text)
    for k,l in enumerate(token):
        if len(token[k]) > 7 and (token[k] in words.words()) != bool("True"):
            spn = [x for x in wordninja.split(token[k])]
            if len(spn) > 1 :
                for ls in range(0,len(spn)):
                    c = "True"
                    if (bool(c) == (spn[ls] in words.words()) and len(spn[ls]) > 2):            
                        spn1 = spn[ls].replace(spn[ls],' ' + spn[ls] + ' ')
                        spn[ls] = spn1
                        
                    else:
                        spn1 = ''
                        spn[ls] = spn1
                    token[k] = ' '.join([w for w in spn if w not in STOP_WORDS])                  
                    

    text1 = ' '.join([w for w in token if len(w) > 1])
  
    return text1

  
def spl(text):
    token = word_tokenize(text)
    for k,l in enumerate(token):
        if len(token[k]) > 5 :
            spn = [x for x in wordninja.split(token[k])]
            if len(spn) > 1 :
                for ls in range(0,len(spn)):
                    if len(spn[ls]) > 2:            
                        spn1 = spn[ls].replace(spn[ls],' ' + spn[ls] + ' ')
                        spn[ls] = spn1
                    else:
                        spn1 = ''
                        spn[ls] = spn1
                        
                    token[k] = ' '.join([w for w in spn if w not in STOP_WORDS])                  

    text1 = ' '.join([w for w in token if len(w) > 1])
  
    return text1

# -*- coding: utf-8 -*-
"""preprocess

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yro1k1lO0cuWUU89L9JfXPEtsIi4xF-2
"""

def preprocess(text):

  #convert unicode to ascii 

  w = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
  
  #translate any language to english

  translator = Translator()
  det = translator.detect(w)
  strdet = str(det)
  lng=str(strdet[14:16])
  if lng != "en" :
    translations = translator.translate(w,dest='en')
    w = translations.text
  else:
    pass

  #to convert the text to lower case 

  w = w.lower()

  #to remove email from the description
  w = re.sub(r'([a-z0-9]+[\._]?[a-z0-9]+[@]\w+[.]\w{2,3})',' ',w)

  #To remove the disclaimer
  
  w = w.replace("this communication is intended solely for the use of the addressee and may contain information that is sensitive, confidential or exempt from disclosure under applicable law. it is strictly forbidden to distribute, distribute or reproduce this communication by anyone other than the intended recipient. if you have received this message by mistake, please notify the sender and delete this message.",'')
  
  w = w.replace("select the following link to view the disclaimer in an alternate language.",' ')
  
  w = w.replace("personally liable partner / general partner: company gmbh",' ')
  
  w = w.replace("managing directors: phvkowml azbtkqwx, naruedlk mpvhakdq registered office: Germany",' ')
 
  #to remove \r,\n,. from the 
  rep = ["\r","\n","subject","sent","received","hi ","hello","first","last","name","from","afternoon","evening","morning","yesterday","today","second"]

  for i,j in enumerate(rep):
    w = w.replace(rep[i],' ')  
    
  #to remove data timestamp from the text
  #w = re.sub(r'([0]\d|[1][0-2])\/([0-2]\d|[3][0-1])\/([2][01]|[1][6-9])\d{2}|([\s,/]([0-1]\d|[2][0-3])(\:[0-5]\d){1,2})?','',w)

   #to expand the word's with single quotes

  negations_dic = {"isn't":"is not", "aren't":"are not", "wasn't":"was not", "weren't":"were not",
                "haven't":"have not","hasn't":"has not","hadn't":"had not","won't":"will not",
                "wouldn't":"would not", "don't":"do not", "doesn't":"does not","didn't":"did not",
                "can't":"can not","couldn't":"could not","shouldn't":"should not","mightn't":"might not",
                "mustn't":"must not","n't":"not","i'm":"iam",}

  neg_pattern = re.compile(r'\b(' + '|'.join(negations_dic.keys()) + r')\b')
  w = neg_pattern.sub(lambda x: negations_dic[x.group()], w)

  #to remove special characters,digits and symbols 

  w = re.sub(r'(\W|[0-9])',' ',w)
  
  # define punctuation
  punctuations = '''!()-[]{};:'"\,<>./?@#$%^&*_~'''
  
  # To take input from the user
  # remove punctuation from the string
  no_punct = ""
  for char in w:
   if char not in punctuations:
       no_punct = no_punct + char

  w = no_punct
  
  #to seperate combined word and nullify meanungless words
  #w = spl_comb_string(w)

  #to lemmatize the give sentence and remove stop words

  nlp = spacy.load('en', disable=['parser', 'ner'])

  doc = nlp(w)

  w = " ".join([token.lemma_ for token in doc if token not in STOP_WORDS and len(token) > 2 ])
  
  w = w.replace("PRON",' ')
  
  #to remove duplicate words from the sentence

  l = w.split()
  k = [] 
  for i in l: 
    # If condition is used to store unique string  
    # in another list 'k'  
    if (w.count(i)>1 and (i not in k)or w.count(i)==1): 
      k.append(i)  
  w = ' '.join(k)

  # remove extra whitespace
  w = re.sub(' +', ' ', w)
  
  return w